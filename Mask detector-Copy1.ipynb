{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.48.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split-folders tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3833 files [02:12, 28.85 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "import os\n",
    "\n",
    "splitfolders.ratio('E:/Data Science and ML/Mask detector/dataset 2/Face-Mask-Detection-master/dataset/', output='hasil_split', seed=1337, ratio=(.6, .4))\n",
    "hasil = 'hasil_split'\n",
    "train_dir = os.path.join(hasil, 'train')\n",
    "validation_dir = os.path.join(hasil, 'val')\n",
    "\n",
    "train_withmask_dir = os.path.join(train_dir, 'with_mask')\n",
    "train_withoutmask_dir = os.path.join(train_dir, 'without_mask')\n",
    "\n",
    "\n",
    "val_withmask_dir = os.path.join(validation_dir, 'with_mask')\n",
    "val_withoutmask_dir = os.path.join(validation_dir, 'without_mask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2887 images belonging to 2 classes.\n",
      "Found 1979 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#TRAINING_DIR = \"E:/Data Science/Mask detector/observations-master/experiements/dest_folder/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=32, \n",
    "                                                    target_size=(150, 150))\n",
    "#VALIDATION_DIR = \"E:/Data Science/Mask detector/observations-master/experiements/dest_folder/test\"\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, \n",
    "                                                        color_mode=\"rgb\",\n",
    "                                                         batch_size=32, \n",
    "                                                         target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(lr=1e-4, decay=1e-4 / 32)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4852 - acc: 0.7558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "91/91 [==============================] - 722s 8s/step - loss: 0.4852 - acc: 0.7558 - val_loss: 0.3393 - val_acc: 0.8757\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3133 - acc: 0.8791 INFO:tensorflow:Assets written to: model2-002.model\\assets\n",
      "91/91 [==============================] - 2449s 27s/step - loss: 0.3133 - acc: 0.8791 - val_loss: 0.3208 - val_acc: 0.8772\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3075 - acc: 0.8791INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "91/91 [==============================] - 669s 7s/step - loss: 0.3075 - acc: 0.8791 - val_loss: 0.2994 - val_acc: 0.8903\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2956 - acc: 0.8826INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "91/91 [==============================] - 639s 7s/step - loss: 0.2956 - acc: 0.8826 - val_loss: 0.2757 - val_acc: 0.8959\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 655s 7s/step - loss: 0.2811 - acc: 0.8978 - val_loss: 0.2808 - val_acc: 0.8974\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 677s 7s/step - loss: 0.2651 - acc: 0.8978 - val_loss: 0.2763 - val_acc: 0.8944\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2609 - acc: 0.9079INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "91/91 [==============================] - 684s 8s/step - loss: 0.2609 - acc: 0.9079 - val_loss: 0.2645 - val_acc: 0.8944\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 676s 7s/step - loss: 0.2602 - acc: 0.9016 - val_loss: 0.2651 - val_acc: 0.8999\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 1422s 16s/step - loss: 0.2491 - acc: 0.9065 - val_loss: 0.2771 - val_acc: 0.8898\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2345 - acc: 0.9086INFO:tensorflow:Assets written to: model2-010.model\\assets\n",
      "91/91 [==============================] - 759s 8s/step - loss: 0.2345 - acc: 0.9086 - val_loss: 0.2627 - val_acc: 0.8924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "model=load_model(\"E:\\Data Science and ML\\Mask detector\\model2-008.model\")\n",
    "\n",
    "results={0:'without mask',1:'use mask'}\n",
    "GR_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "rect_size = 4\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "\n",
    "haarcascade = cv2.CascadeClassifier('E:\\Data Science and ML\\Mask detector\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = cap.read()\n",
    "    im=cv2.flip(im,1,1) \n",
    "\n",
    "    \n",
    "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\n",
    "    faces = haarcascade.detectMultiScale(rerect_size)\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * rect_size for v in f] \n",
    "        \n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        rerect_sized=cv2.resize(face_img,(150,150))\n",
    "        normalized=rerect_sized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),GR_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),GR_dict[label],-1)\n",
    "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    \n",
    "    if key == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
